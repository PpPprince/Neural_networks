{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCiE0-3kZHX2"
      },
      "source": [
        "# Programming a neural network layer\n",
        "\n",
        "[Keras](https://keras.io) is a high-level deep-learning framework building on top of [TensorFlow](https://www.tensorflow.org). These frameworks follow the _symbol-to-symbol derivatives_ approach, i.e. automatically derive a computational graph to calculate derivatives. You just need to declare your inputs as TensorFlow variables and use TensorFlow operations on them to compute the forward pass.  \n",
        "\n",
        "## Task 6.1\n",
        "\n",
        "Work through the [Keras tutorial on custom layers](https://keras.io/guides/making_new_layers_and_models_via_subclassing) to learn how to create your own neural network layer.  \n",
        "Create a custom Keras layer that computes Gaussian basis functions, i.e. a layer that maps an input vector $\\mathbf x \\in \\mathbb R^n$ onto an output vector $\\mathbf y = f(\\mathbf x) \\in \\mathbb R^m$ as follows:\n",
        "\\begin{align}\n",
        "  f: \\mathbf x \\in \\mathbb R^n \\mapsto \\left[w_i \\exp\\left(-\\frac{\\|\\mathbf x - \\boldsymbol\\mu_i\\|^2}{\\sigma_i^2}\\right)\\right]_{i=1..m} \\in \\mathbb R^m\n",
        "\\end{align}\n",
        "\n",
        "Instead of projecting an input $\\mathbf x$ onto a weight vector $\\mathbf w$ as the standard neuron function $f(\\mathbf x) = \\sigma(\\mathbf w \\cdot \\mathbf x + b)$ does, the Gaussian basis function becomes active (with weight $w_i$) for all inputs $\\mathbf x$ close to a prototype $\\boldsymbol \\mu_i$. This activation quickly decays with increasing distance of $\\mathbf x$ to $\\boldsymbol \\mu_i$. The parameter $\\sigma_i$ controls the width of the Gaussian, i.e. the size of the active region.\n",
        "\n",
        "For efficient tensor-based operations you need to correctly _broadcast_ the tensors for the difference operation: TensorFlow will pass an input matrix of shape `(batch size, input dim)` for $\\mathbf X$, while you will have a matrix of centers $\\boldsymbol \\mu$ of shape `(input dim, #units)`. To correctly [broadcast](https://numpy.org/doc/stable/user/basics.broadcasting.html) them together, you will need Keras' [`expand_dims()`](https://www.tensorflow.org/api_docs/python/tf/keras/backend/expand_dims) function to extend $\\mathbf X$'s shape to `(batch size, input dim, 1)`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENMHKe3pZHX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a484de-ba52-47e9-cf4e-e59ba2f04885"
      },
      "source": [
        "import numpy\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class RBFLayer(keras.layers.Layer):\n",
        "    def __init__(self,units,gamma, **kwargs):\n",
        "        super(RBFLayer, self).__init__(**kwargs)\n",
        "        self.units=units\n",
        "        self.gamma=gamma\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        #super(RBFLayer, self).build(input_shape)\n",
        "        self.mu=self.add_weight(name='mu',shape=(int(input_shape[1]),self.units),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "        self.w=self.add_weight(name='w',\n",
        "                               shape=(self.units,),\n",
        "                               initializer='uniform',\n",
        "                               trainable=True)\n",
        "\n",
        "    def call(self, X):\n",
        "        diff=K.expand_dims(X)-self.mu\n",
        "        l2=K.sum(K.pow(diff,2),axis=1)\n",
        "        res=K.exp(-1*self.gamma *l2)\n",
        "        X=res*self.w\n",
        "        return X\n",
        "\n",
        "\n",
        "class InitCentersRandom(keras.initializers.Initializer):\n",
        "    \"\"\" Initializer to initialize centers of RBF network from random samples of the data set.\"\"\"\n",
        "\n",
        "    def __init__(self, X):\n",
        "        self.X = X\n",
        "\n",
        "    def __call__(self, shape, dtype=None):\n",
        "        idx=np.random.randint(self.X.shape[0],size=shape[0])\n",
        "        #pass\n",
        "        return self.X[idx,:]\n",
        "\n",
        "\n",
        "\n",
        "X = tf.ones((3, 5))  # input tensor X with batch dimension 3 and data dim N=5\n",
        "#mu = tf.ones((5, 2))  # tensor mu with data dim N=5 and 2 units\n",
        "#diffs = K.expand_dims(X) - mu  # diffs tensor: 3 x 5 x 2\n",
        "print(X.shape)\n",
        "print(X)\n",
        "layer=RBFLayer(2,gamma=1)\n",
        "Y=layer(X)\n",
        "print(Y.shape)\n",
        "print(Y)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 5)\n",
            "tf.Tensor(\n",
            "[[1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1.]], shape=(3, 5), dtype=float32)\n",
            "(3, 2)\n",
            "tf.Tensor(\n",
            "[[-6.6189721e-05  2.0400532e-04]\n",
            " [-6.6189721e-05  2.0400532e-04]\n",
            " [-6.6189721e-05  2.0400532e-04]], shape=(3, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of above code is the shape of resulting tensor when applying the RBFLayer to input tensor 'X'. Given that we are instantiating the 'RBFLayer' with 2 units and our input tensor 'X' has a batch size of 3, the output is in the shape of resulting tensor, which should be '(3,2)'. This also means the layer is processing the 3 input samples through 2 neurons int he RBFLayer, so we are getting 2 outputs for each inoput sample."
      ],
      "metadata": {
        "id": "r-YKa-oHwAb4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4LhftLcZHX5"
      },
      "source": [
        "## Task 6.2\n",
        "\n",
        "Compare the performance of such a Gaussian basis function layer with that of a standard [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer on the MNIST dataset.  \n",
        "Hint: Utilize existing tutorials on setting up your first MNIST MLP with Keras, e.g. https://www.tensorflow.org/guide/keras/train_and_evaluate.\n",
        "\n",
        "To achieve decent performance, you want to:\n",
        "- Initialize the centers $\\boldsymbol \\mu_i$ from random data samples $\\mathbf x$ (create a custom [initializer](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/Initializer))\n",
        "- Initialize $\\sigma_i$ to the typical in-class distance between data points.  \n",
        "  Use [`scipy.spatial.distance_matrix`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance_matrix.html) to compute this statistics on a random selection of your input data.  \n",
        "  (Doing it on the full dataset will probably exhaust your memory.)\n",
        "- Initialize $w_i = 1$\n",
        "\n",
        "Questions:\n",
        "- How many parameters each of those networks have?\n",
        "- Which network trains faster / easier?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's define functions to create model with 'Dense' layers and 'RBFLayer'\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "def create_dense_model(input_shape,num_classes):\n",
        "  model=Sequential()\n",
        "  model.add(Flatten(input_shape=input_shape))\n",
        "  model.add(Dense(64,activation='relu'))\n",
        "  model.add(Dense(64,activation='relu'))\n",
        "  model.add(Dense(num_classes,activation='softmax'))\n",
        "  return model\n",
        "\n",
        "\n",
        "def create_rbf_model(input_shape,num_classes,units=64,gamma=1.0):\n",
        "  model=Sequential()\n",
        "  model.add(Flatten(input_shape=input_shape))\n",
        "  model.add(RBFLayer(units,gamma))\n",
        "  model.add(Dense(num_classes,activation='softmax'))\n",
        "  return model"
      ],
      "metadata": {
        "id": "6rV_hDEMwsOL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading MNIST dataset and normalizing the images:\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images,train_labels),(test_images,test_labels)= mnist.load_data()\n",
        "\n",
        "train_images=train_images/255.0\n",
        "test_images=test_images/255.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQlwkGYwyFuk",
        "outputId": "9c10823d-1677-4f01-dfb5-fcb695517b09"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating models and compiling them:\n",
        "num_classes=10\n",
        "input_shape=(28,28)\n",
        "\n",
        "dense_model=create_dense_model(input_shape,num_classes)\n",
        "rbf_model=create_rbf_model(input_shape,num_classes)\n",
        "\n",
        "dense_model.compile(optimizer='adam',\n",
        "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "rbf_model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "tkKd4DkUybHM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the model:\n",
        "import time \n",
        "start_dense_time=time.time()\n",
        "dense_history=dense_model.fit(train_images,train_labels,epochs=5,\n",
        "                              validation_data=(test_images,test_labels))\n",
        "end_dense_time=time.time()\n",
        "start_rbf_time=time.time()\n",
        "rbf_history=rbf_model.fit(train_images, train_labels, epochs=5,\n",
        "                          validation_data=(test_images,test_labels))\n",
        "end_rbf_time=time.time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p54lDC9JzZKW",
        "outputId": "89900877-3474-4f86-9cdd-d38dd9333628"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 12s 4ms/step - loss: 0.2811 - accuracy: 0.9178 - val_loss: 0.1540 - val_accuracy: 0.9539\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1253 - accuracy: 0.9622 - val_loss: 0.1145 - val_accuracy: 0.9640\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0915 - accuracy: 0.9713 - val_loss: 0.1134 - val_accuracy: 0.9642\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0719 - accuracy: 0.9774 - val_loss: 0.0843 - val_accuracy: 0.9731\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0593 - accuracy: 0.9810 - val_loss: 0.0890 - val_accuracy: 0.9744\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 2.3015 - accuracy: 0.1119 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many parameters each of those networks have?"
      ],
      "metadata": {
        "id": "2gVUnNkY000P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this can be found using the count_params() function of the model after it has been built.\n",
        "print(\"Dense model parameters: \", dense_model.count_params())\n",
        "print(\"RBF model parameters: \", rbf_model.count_params())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgnnAHtS03HL",
        "outputId": "ecbed9d4-5d01-4a88-c833-a65a9fdc230f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense model parameters:  55050\n",
            "RBF model parameters:  50890\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which network trains faster / easier?"
      ],
      "metadata": {
        "id": "wIhN7x202Ldt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dense Model Training Time: \", end_dense_time - start_dense_time)\n",
        "print(\"RBF Model Training Time: \", end_rbf_time - start_rbf_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c544MmFT4PWb",
        "outputId": "bc15b98a-64ce-46e5-d13a-ccc8ceff7317"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense Model Training Time:  42.06096053123474\n",
            "RBF Model Training Time:  28.53823161125183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "given that Dense Layer involves straightforward matrix multiplications, while the RBF layer requires additional computation fo rthe gaussian function (including exponentials), so we cna say network with dense layer woul dgenerally train faster than RBL layer netowrk of similar size. Also, as you can observe in above two code snippets, parameters in rbf model is less so it is taking less time and trained faster than dense layer netowrk."
      ],
      "metadata": {
        "id": "fBzP77EP3Rjf"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}