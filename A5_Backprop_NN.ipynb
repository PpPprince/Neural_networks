{
  "cells": [
    {
      "source": [
        "## Task 5.1 Gradients are row vectors\n",
        "\n",
        "Consider a function $f : \\mathbb R^{n×1} → \\mathbb R^{m×1}$ mapping n-dimensional column vectors onto m-dimensional column vectors.\n",
        "\n",
        "For any $\\mathbf x \\in \\mathbb R^{n×1}$, the derivative $f'(\\mathbf x)$ is characterized by its linear approximation property\n",
        "$$ f(\\mathbf x + \\mathbf h) \\approx f(\\mathbf x) + f'(\\mathbf x) \\cdot \\mathbf h $$\n",
        "for small $\\mathbf h \\in \\mathbb R^{n×1}$.\n",
        "\n",
        "Considering the dimension of the involved entities $\\mathbf x$, $\\mathbf h$, $f(\\mathbf x)$, and $f'(\\mathbf x)$, explain why the gradient, i.e. the derivative of a scalar function ($m=1$), is a row vector.\n",
        "\n",
        "---\n",
        "Solution: Considering the dimensions of the involved entities and the linear approximation property. For the property to hold we can look at the dimensions of each of the involved summation terms. $f(\\mathbf{x}) \\in \\mathbb R^{m×1}$ and we have $\\mathbf h \\in \\mathbb R^{n×1}$ vector multiplication rules only allow  $f'(\\mathbf x) \\in R^{1xn}$. Hence the gradient can only represent a row vector for the derivative of a scalar function $(m=1)$. "
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui6D367hwX77"
      }
    },
    {
      "source": [
        "## Task 5.2 Gradient of a Bilinear form\n",
        "\n",
        "Let $\\mathbf x \\in \\mathbb R^n$ and $\\mathbf y \\in \\mathbb R^m$ be usual column vectors.\n",
        "The bilinear form $f(\\mathbf x, \\mathbf y, W) = \\mathbf x^t W \\mathbf y = \\sum_i x_i \\sum_j w_{ij} y_j$ yields a scalar.\n",
        "\n",
        "1. What's the correct dimension of $W$?\n",
        "2. Determine the dimension of the following derivatives:\n",
        "   1. $\\nabla_{\\mathbf x} f$  (standard row-vector gradient)\n",
        "   2. $\\nabla_{\\mathbf x^t} f \\equiv \\nabla^t_{\\mathbf x} f$  (column-vector gradient)\n",
        "   3. $\\nabla_{\\mathbf y} f$  (row-vector gradient)\n",
        "   4. $\\nabla_{\\mathbf y^t} f \\equiv \\nabla^t_{\\mathbf y} f$  (column-vector gradient)\n",
        "3. Compute the derivatives\n",
        "\n",
        "---\n",
        "1. The matrix $W$ should have dimensions $n \\times m$ in order for the bilinear form $f(\\mathbf x, \\mathbf y, W) = \\mathbf x^t W \\mathbf y$ to be well-defined. \n",
        "\n",
        "2. The dimensions of the derivatives are as follows:\n",
        "\n",
        "   A. $\\nabla_{\\mathbf x} f$: The derivative with respect to $\\mathbf x$ will have the same dimensions as $\\mathbf x$, which is an $n$-dimensional column vector.\n",
        "\n",
        "   B. $\\nabla_{\\mathbf x^t} f \\equiv \\nabla^t_{\\mathbf x} f$: The derivative with respect to $\\mathbf x^t$ will have the same dimensions as $\\mathbf x^t$, which is a $1 \\times n$ row vector.\n",
        "\n",
        "   C. $\\nabla_{\\mathbf y} f$: The derivative with respect to $\\mathbf y$ will have the same dimensions as $\\mathbf y$, which is an $m$-dimensional column vector.\n",
        "\n",
        "   D. $\\nabla_{\\mathbf y^t} f \\equiv \\nabla^t_{\\mathbf y} f$: The derivative with respect to $\\mathbf y^t$ will have the same dimensions as $\\mathbf y^t$, which is a $1 \\times m$ row vector.\n",
        "\n",
        "3. Computing the derivatives:\n",
        "\n",
        "   A. $\\nabla_{\\mathbf x} f = \\frac{\\partial f}{\\partial \\mathbf x} = W\\mathbf y$\n",
        "\n",
        "   B. $\\nabla_{\\mathbf x^t} f \\equiv \\nabla^t_{\\mathbf x} f = \\frac{\\partial f}{\\partial \\mathbf x^t} = \\mathbf y^t W^t $\n",
        "\n",
        "   C. $\\nabla_{\\mathbf y} f = \\frac{\\partial f}{\\partial \\mathbf y} = W^t \\mathbf x$\n",
        "\n",
        "   D. $\\nabla_{\\mathbf y^t} f \\equiv \\nabla^t_{\\mathbf y} f = \\frac{\\partial f}{\\partial \\mathbf y^t} = \\mathbf x^t W$"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "KjtYaOm7wX8A"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Smid-oMwX8B"
      },
      "source": [
        "## Task 5.3 Computation Graph\n",
        "\n",
        "Consider the following computational graph:\n",
        "\n",
        "![https://github.com/rhaschke/Neural-Networks/blob/master/backprop.svg](https://raw.githubusercontent.com/rhaschke/Neural-Networks/master/backprop.svg)\n",
        "\n",
        "1. Write the computational graph as a formula: $y = \\| e \\|^2_2 = \\|z + u \\|^2_2 = \\|w \\cdot x + (-1 \\cdot t)\\|^2_2$\n",
        "\n",
        "2. Perform a forward pass for the graph, starting with the given values for $\\mathbf W$, $\\mathbf x$, and $\\mathbf t$.\n",
        "Denote the results directly in the graph, above the connecting arrow lines.\n",
        "\n",
        "![forward prop graph](https://drive.google.com/uc?id=1-bjcXG3PxzXzOrqNX_wvMsRbEpqcuc0T)\n",
        "\n",
        "3. Determine the local gradients of all operation nodes, in component-wise notation, i.e.:\n",
        "\\begin{align}\n",
        "+: &\\qquad \\frac{\\partial e_i}{\\partial z_j} = \\delta_{ij} &\\quad &\\frac{\\partial e_i}{\\partial u_j} = \\delta_{ij} \\\\\n",
        "\\|\\cdot\\|^2: &\\qquad \\frac{\\partial y}{\\partial e_i} = 2e_i \\\\\n",
        "\\times: &\\qquad \\frac{\\partial z_k}{\\partial w_{ij}} = \\delta_{ki}x_j &\\quad &\\frac{\\partial z_i}{\\partial x_j} = w_{i,j} &\\\\\n",
        "\\times\\text{-}1: &\\qquad \\frac{\\partial u_i}{\\partial t_j} = \\delta_{ij} &\\\\\n",
        "\\end{align}\n",
        "\n",
        "4. Perform a full backward-pass, denoting the results in the graph below the connecting arrow lines.\n",
        "\n",
        "Using the chain rule and the patterns in gradient flow. We achieve the following backpropagation graph:\n",
        "\n",
        "![backprop graph](https://drive.google.com/uc?id=1BtSvnsqhJkpOI5KkqIdARyhssfUxI_WE)\n",
        "\n",
        "where:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial y}{\\partial W} &= \\frac{\\partial y}{\\partial e}\\frac{\\partial e}{\\partial z}\\frac{\\partial z}{\\partial W} = 2e \\cdot x^t\\\\\n",
        "\\frac{\\partial y}{\\partial x} &= \\frac{\\partial y}{\\partial e}\\frac{\\partial e}{\\partial z}\\frac{\\partial z}{\\partial x} = 2W^t \\cdot e\\\\\n",
        "\\frac{\\partial y}{\\partial t} &= \\frac{\\partial y}{\\partial e}\\frac{\\partial e}{\\partial u}\\frac{\\partial u}{\\partial t} = -2e\n",
        "\\end{align}\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.9 64-bit"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}